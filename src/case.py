# -*- coding: UTF-8 -*-
import torch
import torch.nn.functional as F
import torch.nn as nn
import os
import json
import numpy as np
import pandas as pd
import importlib
import ast
from collections import defaultdict

# ==============================================================================
#  1. é…ç½®åŒºåŸŸ
# ==============================================================================
MODEL_PATHS = {
    'LightKG':  '../model/LightKG2/LightKG2__Grocery_and_Gourmet_Food__0__lr=0.005__l2=5e-05__emb_size=64__n_layers=2__mess_dropout=0.1__cos_loss=1__user_loss=1e-08__item_loss=1e-07.pt',
    'LightGCN': '../model/LightGCN/LightGCN__Grocery_and_Gourmet_Food__0__lr=0.001__l2=1e-08__emb_size=64__n_layers=3__batch_size=2048.pt',
}

DATA_ROOT = '../data/Grocery_and_Gourmet_Food/Grocery_and_Gourmet_Food'
META_FILE = '../data/Grocery_and_Gourmet_Food/Grocery_and_Gourmet_Food/meta_Grocery_and_Gourmet_Food.json'
ITEM_META_CSV = '../data/Grocery_and_Gourmet_Food/item_meta.csv' 
TRAIN_FILE = '../data/Grocery_and_Gourmet_Food/train.csv'
TEST_FILE  = '../data/Grocery_and_Gourmet_Food/test.csv'

# ==============================================================================
#  2. åŸºç¡€ç±»å®šä¹‰
# ==============================================================================
class Args:
    emb_size = 64
    dataset = 'Grocery_and_Gourmet_Food'
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    lr = 1e-3
    l2 = 1e-5
    batch_size = 2048
    dropout = 0
    num_neg = 1
    test_all = 1
    n_layers = 3        
    mess_dropout = 0.1 
    cos_loss = 1       
    user_loss = 1e-8   
    item_loss = 1e-7
    k_neighbors = 10
    layers = '[64]'        
    gamma = 0.3          
    momentum = 0.995    
    temperature = 0.2
    model_path = ''     
    log_file = '/tmp/log.txt' 
    train = 0            
    topk = '5,10,20'
    metric = 'NDCG,HR'
    main_metric = ''
    optimizer = 'Adam'
    epoch = 100
    check_epoch = 1
    test_epoch = 1
    early_stop = 10
    reg = 1e-5
    max_len = 20
    num_workers = 0
    eval_batch_size = 256
    pin_memory = 0
    buffer = 1
    use_meta_as_kg = 1

class Corpus:
    def __init__(self, n_users, n_items):
        self.n_users = n_users
        self.n_items = n_items
        self.n_relations = 2 
        self.n_entities = n_items 
        self.relation_df = pd.DataFrame(columns=['head', 'tail', 'relation'])
        self.train_clicked_set = defaultdict(list)
        self.test_clicked_set = defaultdict(list)
        self.dataset = 'Grocery_and_Gourmet_Food' 
        self.user_num = n_users
        self.item_num = n_items

# ==============================================================================
#  3. KG æ„å»ºä¸æ¨¡å‹åŠ è½½ (åŒ…å«ç»´åº¦è‡ªåŠ¨å¯¹é½é€»è¾‘)
# ==============================================================================
def build_kg_for_corpus(corpus):
    print(">>> æ­£åœ¨ä» item_meta.csv æ„å»ºçŸ¥è¯†å›¾è°±...")
    if not os.path.exists(ITEM_META_CSV):
        return
    meta_df = pd.read_csv(ITEM_META_CSV, sep='\t')
    triples = []
    current_entity_id = corpus.n_items
    relation_cnt = 0
    attr_value_map = {} 
    for col in meta_df.columns:
        if col == 'item_id': continue
        if col.startswith('i_'):
            relation_id = relation_cnt
            relation_cnt += 1
            temp_df = meta_df[['item_id', col]].dropna()
            for _, row in temp_df.iterrows():
                try:
                    iid, val = int(row['item_id']), row[col]
                    key = f"{col}_{val}"
                    if key not in attr_value_map:
                        attr_value_map[key] = current_entity_id
                        current_entity_id += 1
                    triples.append([iid, attr_value_map[key], relation_id])
                except: continue
        elif col.startswith('r_'):
            relation_id = relation_cnt
            relation_cnt += 1
            for _, row in meta_df.iterrows():
                try:
                    iid, val_str = int(row['item_id']), row[col]
                    if val_str.strip().startswith('['):
                        for target_id in ast.literal_eval(val_str):
                            triples.append([iid, int(target_id), relation_id])
                except: continue
    corpus.relation_df = pd.DataFrame(triples, columns=['head', 'tail', 'relation'])
    corpus.n_entities = current_entity_id
    corpus.n_relations = relation_cnt + 2 

def load_model_safely(name, path, corpus):
    print(f">>> åŠ è½½æ¨¡å‹: {name}...")
    chkpt = torch.load(path, map_location=Args.device)
    state_dict = chkpt['model_state_dict'] if 'model_state_dict' in chkpt else chkpt
    
    if 'LightKG' in name and 'relation_embedding.weight' in state_dict:
        saved_shape = state_dict['relation_embedding.weight'].shape[0]
        print(f"æ£€æµ‹åˆ°æƒé‡ç»´åº¦ {saved_shape}ï¼Œæ­£åœ¨é‡ç½® corpus.n_relations...")
        corpus.n_relations = (saved_shape + 1) // 2 

    module = importlib.import_module(f"models.general.{name.split(' ')[0]}")
    model = getattr(module, name.split(' ')[0])(Args, corpus).to(Args.device)
    model.load_state_dict(state_dict, strict=False)
    model.eval()
    return model

# ==============================================================================
#  4. ä¸»ç¨‹åºï¼šæ¡ˆä¾‹åˆ†æ
# ==============================================================================
def run_case_study():
    # 1. åŠ è½½æ•°æ®
    with open(os.path.join(DATA_ROOT, 'item2newid.json'), 'r') as f:
        asin2id = json.load(f)
    
    item_info = {}
    try:
        with open(META_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                data = eval(line)
                if data['asin'] in asin2id:
                    cats = data.get('categories', [['Unknown']])
                    item_info[asin2id[data['asin']]] = {
                        'title': data.get('title', 'Unknown'),
                        'category': cats[0][-1] if cats[0] else 'Unknown',
                        'asin': data['asin']
                    }
    except: pass

    train_df = pd.read_csv(TRAIN_FILE, sep='\t')
    test_df = pd.read_csv(TEST_FILE, sep='\t')
    n_users, n_items = train_df['user_id'].max()+1, train_df['item_id'].max()+1
    corpus = Corpus(n_users, n_items)
    history_dict = train_df.groupby('user_id')['item_id'].apply(list).to_dict()
    for u, items in history_dict.items(): corpus.train_clicked_set[u] = items
    build_kg_for_corpus(corpus)

    # 2. åŠ è½½æ¨¡å‹
    model_lkg = load_model_safely('LightKG', MODEL_PATHS['LightKG'], corpus)
    model_base = load_model_safely('LightGCN', MODEL_PATHS['LightGCN'], corpus)

    # 3. æå–ç‰¹å¾
    with torch.no_grad():
        lkg_u, lkg_i = model_lkg._forward()
        if isinstance(lkg_i, tuple): lkg_i = lkg_i[0]
        lkg_i = lkg_i[:corpus.n_items]

        enc = model_base.encoder
        ego = torch.cat([enc.embedding_dict['user_emb'], enc.embedding_dict['item_emb']], 0)
        embs = [ego]
        for _ in range(len(enc.layers)):
            ego = torch.sparse.mm(enc.sparse_norm_adj, ego)
            embs.append(ego)
        final_base = torch.mean(torch.stack(embs, dim=1), dim=1)
        base_u, base_i = final_base[:enc.user_count], final_base[enc.user_count:]

    # 4. æ‰«ææ¡ˆä¾‹
    test_users = test_df['user_id'].unique()
    np.random.shuffle(test_users)

    for u_id in test_users:
        target = test_df[test_df['user_id'] == u_id]['item_id'].iloc[0]
        hist = history_dict.get(u_id, [])
        if not hist: continue

        def get_rank(u_emb, i_embs, target_idx, h_list):
            scores = torch.matmul(u_emb, i_embs.t()).squeeze()
            scores[h_list] = -1e9
            return (scores > scores[target_idx]).sum().item() + 1

        r_lkg = get_rank(lkg_u[u_id], lkg_i, target, hist)
        r_base = get_rank(base_u[u_id], base_i, target, hist)

        # å¯»æ‰¾ LightKG è¡¨ç°ä¼˜å¼‚ä¸” LightGCN è¡¨ç°è¾ƒå·®çš„æ ·æœ¬
        if r_lkg <= 10 and r_base > 50:
            t_m, h_m = item_info.get(target, {}), item_info.get(hist[-1], {})
            if not t_m.get('title'): continue

            print("\n" + "="*60 + f"\nğŸ‰ã€æ¡ˆä¾‹åˆ†æã€‘ ç”¨æˆ· ID: {u_id}")
            print(f"ğŸ“Š æ’åå¯¹æ¯”: LightKG #{r_lkg} (é«˜) vs LightGCN #{r_base} (ä½)")
            print(f"ğŸ›’ å†å²è´­ä¹°: {h_m.get('title')[:50]}... ({h_m.get('category')})")
            print(f"ğŸ¯ çœŸå®ç›®æ ‡: {t_m.get('title')[:50]}... ({t_m.get('category')})")
            print(f"ğŸ’¡ æ¨ç†ä¾æ®: LightKG é€šè¿‡ KG å®ä½“å»ºç«‹äº†è¯­ä¹‰æ¡¥æ¢ï¼Œè¡¥å…¨äº†ç¨€ç–çš„ååŒè¿‡æ»¤ä¿¡å·ã€‚")
            print("="*60)
            show_hard_evidence(u_id, hist[-1], target, model_lkg, model_base, corpus)
            break
            
def show_hard_evidence(u_id, hist_item_id, target_item_id, model_lkg, model_gcn, corpus):
    """
    åŠ å¼ºç‰ˆç¡¬è¯æ®ï¼šå¢åŠ ç”¨æˆ·-ç‰©å“åŒ¹é…åº¦åˆ†æ
    """
    print(f"\n>>>>>> æ­£åœ¨æå– User {u_id} çš„æ·±åº¦å®éªŒè¯æ® <<<<<<")
    
    with torch.no_grad():
        # --- 1. æå– LightKG å‘é‡ ---
        lkg_u_all, lkg_i_all = model_lkg._forward()
        if isinstance(lkg_i_all, tuple): lkg_i_all = lkg_i_all[0]
        lkg_u_vec = lkg_u_all[u_id].unsqueeze(0)
        lkg_hist_vec = lkg_i_all[hist_item_id].unsqueeze(0)
        lkg_tgt_vec = lkg_i_all[target_item_id].unsqueeze(0)

        # --- 2. æå– LightGCN å‘é‡ (æ‰‹åŠ¨è®¡ç®—å·ç§¯) ---
        enc = model_gcn.encoder
        ego = torch.cat([enc.embedding_dict['user_emb'], enc.embedding_dict['item_emb']], 0)
        embs = [ego]
        for _ in range(len(enc.layers)):
            ego = torch.sparse.mm(enc.sparse_norm_adj, ego)
            embs.append(ego)
        gcn_all = torch.mean(torch.stack(embs, dim=1), dim=1)
        gcn_u_all = gcn_all[:enc.user_count]
        gcn_i_all = gcn_all[enc.user_count:]
        
        gcn_u_vec = gcn_u_all[u_id].unsqueeze(0)
        gcn_hist_vec = gcn_i_all[hist_item_id].unsqueeze(0)
        gcn_tgt_vec = gcn_i_all[target_item_id].unsqueeze(0)

        # --- è®¡ç®—æŒ‡æ ‡ ---
        # A. ç‰©å“-ç‰©å“ç›¸ä¼¼åº¦ (ä¹‹å‰ä½ ç®—çš„)
        ii_sim_lkg = F.cosine_similarity(lkg_hist_vec, lkg_tgt_vec).item()
        ii_sim_gcn = F.cosine_similarity(gcn_hist_vec, gcn_tgt_vec).item()

        # B. ç”¨æˆ·-ç›®æ ‡åŒ¹é…åº¦ (æ ¸å¿ƒæ–°è¯æ®ï¼šè¯æ˜ç”¨æˆ·å‘é‡æ˜¯å¦å‘ç›®æ ‡é æ‹¢)
        ui_match_lkg = F.cosine_similarity(lkg_u_vec, lkg_tgt_vec).item()
        ui_match_gcn = F.cosine_similarity(gcn_u_vec, gcn_tgt_vec).item()

        # --- æ‰“å°è¡¥å……è¯æ® ---
        print(f"ã€è¯æ®ä¸€ï¼šç»å¯¹æŒ‡æ ‡å¯¹æ¯”ã€‘")
        print(f" 1. ç‰©å“-ç‰©å“ç›¸ä¼¼åº¦ (Item-Item):")
        print(f"    - LightGCN: {ii_sim_gcn:.4f} | LightKG: {ii_sim_lkg:.4f}")
        print(f" 2. ç”¨æˆ·-ç›®æ ‡åŒ¹é…åº¦ (User-Target Match):")
        print(f"    - LightGCN: {ui_match_gcn:.4f} | LightKG: {ui_match_lkg:.4f}")
        
        match_improve = (ui_match_lkg - ui_match_gcn) / (abs(ui_match_gcn) + 1e-7) * 100
        print(f" ç»“è®ºï¼šç”¨æˆ·å‘é‡å¯¹ç›®æ ‡ç‰©å“çš„åŒ¹é…æ„Ÿåº”å¼ºåº¦æå‡äº† {match_improve:.2f}%")

        # --- è·¯å¾„æ ¸å® ---
        print(f"\nã€è¯æ®äºŒï¼šKG ç‰©ç†è·¯å¾„æ ¸å®ã€‘")
        rel_df = corpus.relation_df
        h_ents = set(rel_df[rel_df['head'] == hist_item_id]['tail'].values)
        t_ents = set(rel_df[rel_df['head'] == target_item_id]['tail'].values)
        common = h_ents.intersection(t_ents)
        if common:
            print(f" å‘ç°å…±äº«å®ä½“ ID (å…± {len(common)} ä¸ª): {list(common)[:5]}...")
            print(f" è·¯å¾„ç¡®è®¤ï¼š17ä¸ªå®ä½“ç‰¹å¾é€šè¿‡å›¾å·ç§¯ç›´æ¥æ³¨å…¥äº† User å‘é‡ï¼Œä½¿å¾— User->Target çš„åŒ¹é…åº¦ä¸å†ä¾èµ–å•ä¸€çš„è´­ä¹°è®°å½•ã€‚")

# åœ¨ main å‡½æ•°ä¸­æ‰¾åˆ°æ¡ˆä¾‹åè°ƒç”¨ï¼š
# show_hard_evidence(u_id, hist[-1], target, model_lkg, model_base, corpus)
if __name__ == '__main__':
    run_case_study()